import numpy as np
import logging
from gensim.models import Word2Vec
from tokenizers import Tokenizer

# Ustawienie logowania dla gensim
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# --- ŚCIEŻKI DO ARTEFAKTÓW WYGENEROWANYCH W TRAKCIE TRENINGU ---
# TOKENIZER_FILE = "../tokenizer/tokenizers/custom_bpe_tokenizer.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v1-tokenizer.json"
TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v3-tokenizer.json"
OUTPUT_MODEL_FILE = "embedding_word2vec_cbow_model.model"
MIN_COUNT = 2  # używane jedynie w komunikatach diagnostycznych

try:
    print(f"Ładowanie tokenizera z pliku: {TOKENIZER_FILE}")
    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)
except FileNotFoundError:
    print(f"BŁĄD: Nie znaleziono pliku '{TOKENIZER_FILE}'. Upewnij się, że plik istnieje.")
    raise

try:
    print(f"Ładowanie wytrenowanego modelu Word2Vec: {OUTPUT_MODEL_FILE}")
    model = Word2Vec.load(OUTPUT_MODEL_FILE)
except FileNotFoundError:
    print(f"BŁĄD: Plik modelu '{OUTPUT_MODEL_FILE}' nie istnieje. Uruchom najpierw skrypt treningowy.")
    raise

# --- DODANA FUNKCJA: OBLICZANIE WEKTORA DLA CAŁEGO SŁOWA ---

def get_word_vector_and_similar(word: str, tokenizer: Tokenizer, model: Word2Vec, topn: int = 20):
    # Tokenizacja słowa na tokeny podwyrazowe
    # Używamy .encode(), aby otoczyć słowo spacjami, co imituje kontekst w zdaniu
    # Ważne: tokenizator BPE/SentencePiece musi widzieć spację, by dodać prefiks '_'
    encoding = tokenizer.encode(" " + word + " ") 
    word_tokens = [t.strip() for t in encoding.tokens if t.strip()] # Usuń puste tokeny
    
    # Usuwamy tokeny początku/końca sekwencji, jeśli zostały dodane przez tokenizator
    if word_tokens and word_tokens[0] in ['[CLS]', '<s>', '<s>', 'Ġ']:
        word_tokens = word_tokens[1:]
    if word_tokens and word_tokens[-1] in ['[SEP]', '</s>', '</s>']:
        word_tokens = word_tokens[:-1]

    valid_vectors = []
    missing_tokens = []
    
    # 1. Zbieranie wektorów dla każdego tokenu
    for token in word_tokens:
        if token in model.wv:
            # Użycie tokenu ze spacją (np. '_ryż') lub bez (np. 'szlach')
            valid_vectors.append(model.wv[token])
        else:
            # W tym miejscu token może być zbyt rzadki i pominięty przez MIN_COUNT
            missing_tokens.append(token)

    if not valid_vectors:
        # Kod do obsługi, gdy żaden token nie ma wektora
        if missing_tokens:
            print(f"BŁĄD: Żaden z tokenów składowych ('{word_tokens}') nie znajduje się w słowniku (MIN_COUNT={MIN_COUNT}).")
        else:
            print(f"BŁĄD: Słowo '{word}' nie zostało przetworzone na wektory (sprawdź tokenizację).")
        return None, None

    # 2. Uśrednianie wektorów
    # Wektor dla całego słowa to średnia wektorów jego tokenów składowych
    word_vector = np.mean(valid_vectors, axis=0)

    # 3. Znalezienie najbardziej podobnych tokenów
    similar_words = model.wv.most_similar(
        positive=[word_vector],
        topn=topn
    )
    
    return word_vector, similar_words

# --- WERYFIKACJA UŻYCIA NOWEJ FUNKCJI ---

print("\n--- Weryfikacja: Szukanie podobieństw dla całych SŁÓW (uśrednianie wektorów tokenów) ---")

# Przykłady, które wcześniej mogły nie działać
words_to_test = ['wojsko', 'szlachta', 'choroba', 'król'] 

for word in words_to_test:
    word_vector, similar_tokens = get_word_vector_and_similar(word, tokenizer, model, topn=10)
    
    if word_vector is not None:
        print(f"\n10 tokenów najbardziej podobnych do SŁOWA '{word}' (uśrednione wektory tokenów {tokenizer.encode(word).tokens}):")
        # Wyświetlanie wektora (pierwsze 5 elementów)
        print(f"  > Wektor słowa (początek): {word_vector[:5]}...")
        for token, similarity in similar_tokens:
            print(f"  - {token}: {similarity:.4f}")

# --- WERYFIKACJA DLA WZORCA MATEMATYCZNEGO (Analogia wektorowa) ---

tokens_analogy = ['dziecko', 'kobieta']

# Używamy uśredniania wektorów dla tokenów
if tokens_analogy[0] in model.wv and tokens_analogy[1] in model.wv:
    similar_to_combined = model.wv.most_similar(
        positive=tokens_analogy,
        topn=10
    )

    print(f"\n10 tokenów najbardziej podobnych do kombinacji tokenów: {tokens_analogy}")
    for token, similarity in similar_to_combined:
        print(f"  - {token}: {similarity:.4f}")
else:
    print(f"\nOstrzeżenie: Co najmniej jeden z tokenów '{tokens_analogy}' nie znajduje się w słowniku. Pomięto analogię.")