import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from safetensors.torch import save_file
from torch.utils.tensorboard import SummaryWriter
import os

# Config / settings
LOG_DIR = 'runs/xor'
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
writer = SummaryWriter(LOG_DIR)

# torch.manual_seed(17070015169259637632)

seed = torch.initial_seed()  # pobiera aktualny seed RNG PyTorcha
print(f"U≈ºyty seed PyTorch: {seed}")

np.set_printoptions(precision=4, suppress=True)

## 1. Definicja Modelu Sieci Neuronowej

# üî•üî•üî• (kszta≈Çt sieci)

class SimpleXORNet(nn.Module):
    def __init__(self):
        super(SimpleXORNet, self).__init__()
        # Warstwa ukryta 1: 2 wej≈õcia (X) -> 8 neuron√≥w
        self.fc1 = nn.Linear(2, 10)
        # Warstwa ukryta 2: 8 neuron√≥w -> 4 neurony
        self.fc2 = nn.Linear(10, 6)
        # Warstwa wyj≈õciowa: 4 neurony -> 1 wyj≈õcie (Y)
        self.fc3 = nn.Linear(6, 1)

    def forward(self, x):
        # 1. Przej≈õcie przez pierwszƒÖ warstwƒô liniowƒÖ
        x = self.fc1(x)
        # 2. Nieliniowa funkcja aktywacji ReLU
        x = nn.ReLU()(x)
        # 3. Druga warstwa ukryta + ReLU
        x = self.fc2(x)
        x = nn.ReLU()(x)
        # 4. Warstwa wyj≈õciowa (logits)
        x = self.fc3(x)
        # 5. Sigmoid ‚Äì prawdopodobie≈Ñstwo (0‚Äì1)
        x = torch.sigmoid(x)
        return x

## 2. Inicjalizacja Modelu

model = SimpleXORNet()
model_epochs = 0

LEARNING_RATE = 0.5 # üî•üî•üî•

# BCELoss dla klasyfikacji binarnej (u≈ºywamy go po Sigmoidzie)
criterion = nn.BCELoss()

# Optymalizator: Wska≈∫nik uczenia (lr) jest kluczowy, tu ma≈Ça warto≈õƒá
optimizer = optim.SGD(model.parameters(), LEARNING_RATE)

## 3. Przygotowanie Danych i Pƒôtla Treningowa
# Wa≈ºne: PyTorch oczekuje liczb zmiennoprzecinkowych dla wej≈õƒá sieci.

NUM_EPOCHS = 2000 # üî•üî•üî•

# Dane wej≈õciowe (4 pary: [0, 0], [0, 1], [1, 0], [1, 1])
X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])

# Etykiety (oczekiwane wyj≈õcia XOR: 0, 1, 1, 0)
Y = torch.tensor([[0.], [1.], [1.], [0.]])

# üî•üî•üî• wykonujƒÖc ten snippet ponownie "KONTYNUUJESZ" trening modelu o kolejne epoki
# üî•üî•üî• aby wystartowaƒá trening od zera, stw√≥rz nowy model (uruchom POPRZEDNI snippet)

print("--- Rozpoczƒôcie Treningu ---")
for epoch in range(NUM_EPOCHS):
    # Krok 1: Forward Pass (przekazanie danych)
    outputs = model(X)

    # Krok 2: Obliczenie Strady (Loss)
    loss = criterion(outputs, Y)

    # Krok 3: Backward Pass (propagacja wsteczna)
    optimizer.zero_grad() # Zerowanie gradient√≥w przed nowym obliczeniem
    loss.backward()       # Obliczenie gradient√≥w
    optimizer.step()      # Aktualizacja wag modelu

    model_epochs += 1
    # Logowanie postƒôp√≥w co 200 epok
    if (epoch + 1) % 200 == 0:
        print(f'Epoka [{epoch+1}/{NUM_EPOCHS}, all: {model_epochs}], Strata (Loss): {loss.item():.6f}')
        # print(f'   outputs: {outputs.detach().numpy()}')
        writer.add_scalar('Loss', loss.item(), epoch)
        writer.add_histogram('Outputs', outputs.data, epoch)
        writer.add_histogram('Gradients/Layer_FC1_Weights', model.fc1.weight.grad, epoch)
        writer.add_histogram('Gradients/Layer_FC2_Weights', model.fc2.weight.grad, epoch)
        writer.add_histogram('Weights/Layer_F1_Weights', model.fc1.weight.data, epoch)
        writer.add_histogram('Weights/Layer_F2_Weights', model.fc2.weight.data, epoch)

print("--- Trening Zako≈Ñczony ---")

## 4. Ocena i Testowanie

print("\n--- Wyniki Testowe (U≈ºyteczne Obliczenie) ---")

# Wy≈ÇƒÖczenie mechanizmu gradient√≥w, poniewa≈º tylko testujemy
with torch.no_grad():
    predictions = model(X)
    # Konwersja prawdopodobie≈Ñstw (0-1) na konkretne klasy (0 lub 1)
    predicted_classes = (predictions >= 0.5).float()

    # print(f"Wej≈õcia (X):\n{X.numpy()}")
    # print(f"Oczekiwane Wyj≈õcia (Y):\n{Y.numpy().flatten()}")
    # print(f"Predykcje Modelu:\n{predicted_classes.numpy().flatten()}")

    # Sprawdzenie u≈ºyteczno≈õci - czy siƒô nauczyli≈õmy?
    accuracy = (predicted_classes == Y).sum().item() / len(Y)
    print(f"\nDok≈Çadno≈õƒá (Accuracy): {accuracy*100:.2f}%")
    
## 5. Wy≈õwietl model (strukturƒô i parametry)

# print("--- Struktura Sieci (Wbudowane print()) ---")
# print(model)

# print("--- Parametry Modelu ---")
# for name, param in model.named_parameters():
#     if param.requires_grad:
#         print(f"- {name}:\n{param.data.numpy()}")
        
## 6. Zapisz wagi do pliku

# 1. Definicja ≈õcie≈ºki pliku
MODEL_PATH = "xor_model_weights.pth"

# 2. Zapisz tylko S≈ÅOWNIK STAN√ìW (wagi i biasy)
# To jest najlepsza praktyka i odpowiednik "warto≈õci" w safetensors.
torch.save(model.state_dict(), MODEL_PATH)

print(f"Model zapisany jako: {MODEL_PATH}")
# Wygenerowany plik to skompresowany s≈Çownik w formacie binarnym.

print(f"(run tensorboard/venv): tensorboard --logdir={LOG_DIR}")
print(f"(run tensorboard/venv): tensorboard --logdir=runs")
print("\nopen http://localhost:6006/; SCALARS - how loss changed over time; HISTOGRAMS - how gradients distributed over epochs")