import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from safetensors.torch import save_file
from torch.utils.tensorboard import SummaryWriter
import os

# Config / settings
LOG_DIR = 'runs/xor'
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
writer = SummaryWriter(LOG_DIR)

np.set_printoptions(precision=4, suppress=True)

## 1. Definicja Modelu Sieci Neuronowej

# ヰヰ (ksztat sieci)

class SimpleXORNet(nn.Module):
    def __init__(self):
        super(SimpleXORNet, self).__init__()
        # Warstwa ukryta: 2 wejcia (X) -> 4 neurony
        self.fc1 = nn.Linear(2, 4)
        # Warstwa wyjciowa: 4 neurony -> 1 wyjcie (Y)
        self.fc2 = nn.Linear(4, 1)
        # Inicjalizacja Xavier (Glorot) utrzymuje stabilny rozkad na starcie
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.zeros_(self.fc1.bias)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.zeros_(self.fc2.bias)

    def forward(self, x):
        # 1. Przejcie przez pierwsz warstw liniow
        x = self.fc1(x)
        # 2. Zastosowanie nieliniowej funkcji aktywacji ReLU
        x = nn.ReLU()(x)
        # 3. Przejcie przez warstw wyjciow (logits)
        x = self.fc2(x)
        # 4. Zastosowanie Sigmoid do uzyskania prawdopodobiestwa (w zakresie 0-1)
        x = torch.sigmoid(x)
        return x

## 2. Inicjalizacja Modelu

model = SimpleXORNet()
model_epochs = 0

LEARNING_RATE = 0.01  # delikatniejszy krok ogranicza ryzyko plateau

# BCELoss dla klasyfikacji binarnej (u偶ywamy go po Sigmoidzie)
criterion = nn.BCELoss()

# Adam adaptuje krok dla ka偶dej wagi i dodaje stabilizujce momentum
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

## 3. Przygotowanie Danych i Ptla Treningowa
# Wa偶ne: PyTorch oczekuje liczb zmiennoprzecinkowych dla wej sieci.

NUM_EPOCHS = 1600  # lekko du偶ej, by skompensowa mniejszy lr

# Dane wejciowe (4 pary: [0, 0], [0, 1], [1, 0], [1, 1])
X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])

# Etykiety (oczekiwane wyjcia XOR: 0, 1, 1, 0)
Y = torch.tensor([[0.], [1.], [1.], [0.]])

# ヰヰ wykonujc ten snippet ponownie "KONTYNUUJESZ" trening modelu o kolejne epoki
# ヰヰ aby wystartowa trening od zera, stw贸rz nowy model (uruchom POPRZEDNI snippet)

print("--- Rozpoczcie Treningu ---")
for epoch in range(NUM_EPOCHS):
    # Krok 1: Forward Pass (przekazanie danych)
    outputs = model(X)

    # Krok 2: Obliczenie Strady (Loss)
    loss = criterion(outputs, Y)

    # Krok 3: Backward Pass (propagacja wsteczna)
    optimizer.zero_grad() # Zerowanie gradient贸w przed nowym obliczeniem
    loss.backward()       # Obliczenie gradient贸w
    optimizer.step()      # Aktualizacja wag modelu

    model_epochs += 1
    # Logowanie postp贸w co 200 epok
    if (epoch + 1) % 200 == 0:
        print(f'Epoka [{epoch+1}/{NUM_EPOCHS}, all: {model_epochs}], Strata (Loss): {loss.item():.6f}')
        # print(f'   outputs: {outputs.detach().numpy()}')
        writer.add_scalar('Loss', loss.item(), epoch)
        writer.add_histogram('Outputs', outputs.data, epoch)
        writer.add_histogram('Gradients/Layer_FC1_Weights', model.fc1.weight.grad, epoch)
        writer.add_histogram('Gradients/Layer_FC2_Weights', model.fc2.weight.grad, epoch)
        writer.add_histogram('Weights/Layer_FC1_Weights', model.fc1.weight.data, epoch)
        writer.add_histogram('Weights/Layer_FC2_Weights', model.fc2.weight.data, epoch)

print("--- Trening Zakoczony ---")

## 4. Ocena i Testowanie

print("\n--- Wyniki Testowe (U偶yteczne Obliczenie) ---")

# Wyczenie mechanizmu gradient贸w, poniewa偶 tylko testujemy
with torch.no_grad():
    predictions = model(X)
    # Konwersja prawdopodobiestw (0-1) na konkretne klasy (0 lub 1)
    predicted_classes = (predictions >= 0.5).float()

    print(f"Wejcia (X):\n{X.numpy()}")
    print(f"Oczekiwane Wyjcia (Y):\n{Y.numpy().flatten()}")
    print(f"Predykcje Modelu:\n{predicted_classes.numpy().flatten()}")

    # Sprawdzenie u偶ytecznoci - czy si nauczylimy?
    accuracy = (predicted_classes == Y).sum().item() / len(Y)
    print(f"\nDokadno (Accuracy): {accuracy*100:.2f}%")
    
## 5. Wywietl model (struktur i parametry)

print("--- Struktura Sieci (Wbudowane print()) ---")
print(model)

print("--- Parametry Modelu ---")
for name, param in model.named_parameters():
    if param.requires_grad:
        print(f"- {name}:\n{param.data.numpy()}")
        
## 6. Zapisz wagi do pliku

# 1. Definicja cie偶ki pliku
MODEL_PATH = "xor_model_weights.pth"

# 2. Zapisz tylko SOWNIK STANW (wagi i biasy)
# To jest najlepsza praktyka i odpowiednik "wartoci" w safetensors.
torch.save(model.state_dict(), MODEL_PATH)

print(f"Model zapisany jako: {MODEL_PATH}")
# Wygenerowany plik to skompresowany sownik w formacie binarnym.

print(f"(run tensorboard/venv): tensorboard --logdir={LOG_DIR}")
print(f"(run tensorboard/venv): tensorboard --logdir=runs")
print("\nopen http://localhost:6006/; SCALARS - how loss changed over time; HISTOGRAMS - how gradients distributed over epochs")