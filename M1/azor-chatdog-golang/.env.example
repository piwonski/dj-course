# Engine selection: GEMINI or LLAMA_CPP
ENGINE=GEMINI

# === Google Gemini Configuration ===
GEMINI_API_KEY=your_api_key_here
MODEL_NAME=gemini-2.5-flash

# === LLaMA Local Model Configuration (requires llama.cpp) ===
# Uncomment these if using LLAMA_CPP
# ENGINE=LLAMA_CPP
# LLAMA_MODEL_NAME=llama-3.1-8b-instruct
# LLAMA_MODEL_PATH=/path/to/model.gguf
# LLAMA_GPU_LAYERS=1
# LLAMA_CONTEXT_SIZE=2048
